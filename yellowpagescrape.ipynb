{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yellowpagescrape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJAjv+KHG1RNY7qfnMSLKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meenapushpa/micontactscraper/blob/main/yellowpagescrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfPVCnArQdq_"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import requests\n",
        "from lxml import html\n",
        "import unicodecsv as csv\n",
        "import argparse\n",
        "from csv import DictReader\n",
        "import re\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvt_CldMSBey"
      },
      "source": [
        "Import the following above modules using pip command in python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFTOZpC5SOfm"
      },
      "source": [
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5llTL5ABSUOV"
      },
      "source": [
        "Install requests package for warning exceptions for managing website requests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tJ82muuSl1S"
      },
      "source": [
        "def parse_listing(keyword, place):\n",
        "    \"\"\"\n",
        "    Function to process yellowpage listing page\n",
        "    : param keyword: search query\n",
        "    : param place : place name\n",
        "    \"\"\"\n",
        "    url = \"https://www.yellowpages.com/search?search_terms={0}&geo_location_terms={1}\".format(keyword, place)\n",
        "\n",
        "    print(\"retrieving \", url)\n",
        "\n",
        "    headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
        "               'Accept-Encoding': 'gzip, deflate, br',\n",
        "               'Accept-Language': 'en-GB,en;q=0.9,en-US;q=0.8,ml;q=0.7',\n",
        "               'Cache-Control': 'max-age=0',\n",
        "               'Connection': 'keep-alive',\n",
        "               'Host': 'www.yellowpages.com',\n",
        "               'Upgrade-Insecure-Requests': '1',\n",
        "               'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36'\n",
        "               }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6gOC_poSyvt"
      },
      "source": [
        "Adding retries to proces sthe request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "847-jLNySt59"
      },
      "source": [
        "for retry in range(10):\n",
        "        response = requests.get(url, verify=False, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            parser = html.fromstring(response.text)\n",
        "            # making links absolute\n",
        "            base_url = \"https://www.yellowpages.com\"\n",
        "            parser.make_links_absolute(base_url)\n",
        "\n",
        "            XPATH_LISTINGS = \"//div[@class='search-results organic']//div[@class='v-card']\"\n",
        "            listings = parser.xpath(XPATH_LISTINGS)\n",
        "            scraped_results = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7kGGtajS23y"
      },
      "source": [
        "for results in listings:\n",
        "                XPATH_BUSINESS_NAME = \".//a[@class='business-name']//text()\"\n",
        "                XPATH_BUSSINESS_PAGE = \".//a[@class='business-name']//@href\"\n",
        "                XPATH_TELEPHONE = \".//div[@class='phones phone primary']//text()\"\n",
        "                XPATH_ADDRESS = \".//div[@class='info']//div//p[@itemprop='address']\"\n",
        "                XPATH_STREET = \".//div[@class='street-address']//text()\"\n",
        "                XPATH_LOCALITY = \".//div[@class='locality']//text()\"\n",
        "                XPATH_REGION = \".//div[@class='info']//div//p[@itemprop='address']//span[@itemprop='addressRegion']//text()\"\n",
        "                XPATH_ZIP_CODE = \".//div[@class='info']//div//p[@itemprop='address']//span[@itemprop='postalCode']//text()\"\n",
        "                XPATH_RANK = \".//div[@class='info']//h2[@class='n']/text()\"\n",
        "                XPATH_CATEGORIES = \".//div[@class='info']//div[contains(@class,'info-section')]//div[@class='categories']//text()\"\n",
        "                XPATH_WEBSITE = \".//div[@class='info']//div[contains(@class,'info-section')]//div[@class='links']//a[contains(@class,'website')]/@href\"\n",
        "                XPATH_RATING = \".//div[@class='info']//div[contains(@class,'info-section')]//div[contains(@class,'result-rating')]//span//text()\"\n",
        "\n",
        "                raw_business_name = results.xpath(XPATH_BUSINESS_NAME)\n",
        "                raw_business_telephone = results.xpath(XPATH_TELEPHONE)\n",
        "                raw_business_page = results.xpath(XPATH_BUSSINESS_PAGE)\n",
        "                raw_categories = results.xpath(XPATH_CATEGORIES)\n",
        "                raw_website = results.xpath(XPATH_WEBSITE)\n",
        "                raw_rating = results.xpath(XPATH_RATING)\n",
        "                # address = results.xpath(XPATH_ADDRESS)\n",
        "                raw_street = results.xpath(XPATH_STREET)\n",
        "                raw_locality = results.xpath(XPATH_LOCALITY)\n",
        "                raw_region = results.xpath(XPATH_REGION)\n",
        "                raw_zip_code = results.xpath(XPATH_ZIP_CODE)\n",
        "                raw_rank = results.xpath(XPATH_RANK)\n",
        "\n",
        "                business_name = ''.join(raw_business_name).strip() if raw_business_name else None\n",
        "                telephone = ''.join(raw_business_telephone).strip() if raw_business_telephone else None\n",
        "                business_page = ''.join(raw_business_page).strip() if raw_business_page else None\n",
        "                rank = ''.join(raw_rank).replace('.\\xa0', '') if raw_rank else None\n",
        "                category = ','.join(raw_categories).strip() if raw_categories else None\n",
        "                website = ''.join(raw_website).strip() if raw_website else None\n",
        "                rating = ''.join(raw_rating).replace(\"(\", \"\").replace(\")\", \"\").strip() if raw_rating else None\n",
        "                street = ''.join(raw_street).strip() if raw_street else None\n",
        "                if raw_locality:\n",
        "                     locality = ''.join(raw_locality).replace(',\\xa0', '').strip()\n",
        "                     locality, locality_parts = locality.split(',')\n",
        "                     _, region, zipcode = locality_parts.split(' ')\n",
        "                else:\n",
        "                    locality,locality_parts ,region,zipcode=None,None,None,None\n",
        "                business_details = {\n",
        "                        'business_name': business_name,\n",
        "                        'telephone': telephone,\n",
        "                        'category': category,\n",
        "                        'website': website,\n",
        "                        'street': street,\n",
        "                        'locality': locality,\n",
        "                        'region': region,\n",
        "                        'zipcode': zipcode\n",
        "                    }\n",
        "                scraped_results.append(business_details)\n",
        "            return scraped_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej7WQ4IsS8xM"
      },
      "source": [
        "Find the website link,address and phone numbers in yellow pages website by using XPATH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTzNOUoKTGJY"
      },
      "source": [
        "elif response.status_code == 404:\n",
        "            print(\"Could not find a location matching\", place)\n",
        "            # no need to retry for non existing page\n",
        "            break\n",
        "        else:\n",
        "            print(\"Failed to process page\")\n",
        "            return []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4C4QyKOTJr5"
      },
      "source": [
        "If searched results is unable to find it return empty in scraped_data list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3klkbjiLTV8Y"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    with open('\\output.csv', 'r') as read_obj, open('yellowpages-scraped-data.csv', 'wb') as csvfile:\n",
        "            csv_dict_reader = DictReader(read_obj)\n",
        "            fieldnames = [ 'business_name', 'telephone', 'email','category', 'website','street', 'locality', 'region', 'zipcode']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
        "            writer.writeheader()\n",
        "            for row in csv_dict_reader:\n",
        "                name=row['BorrowerName']\n",
        "                address=row['BorrowerAddress']\n",
        "                city=row['BorrowerCity']\n",
        "                zip=row['BorrowerZip']\n",
        "                state=row['BorrowerState']\n",
        "                keyword = name\n",
        "                place = address + city + zip + state\n",
        "                scraped_data = parse_listing(keyword, place)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aueh4OupTlUh"
      },
      "source": [
        "Now, we need to call the parse_listing() function by passing the company name,address as parameter from output.csv in read mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCj1FKmpUBlx"
      },
      "source": [
        "After the results are processed, we need to write the results in separate csv named 'yellowpages-scraped-data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLguA1O2UYlh"
      },
      "source": [
        "                email_headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36'}\n",
        "                if scraped_data:\n",
        "                    updated=True\n",
        "                    if scraped_data[0]['website']:\n",
        "                        try:\n",
        "                            emailresponse = requests.get(scraped_data[0]['website'],verify=False,headers=email_headers)\n",
        "                            if emailresponse.status_code == 200:\n",
        "                                emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", emailresponse.text, re.I))\n",
        "                                for i in emails:\n",
        "                                    scraped_data[0]['email']=i\n",
        "                            elif emailresponse.status_code == 404:\n",
        "                                print(\"Could not find a address field\")\n",
        "                            else:\n",
        "                                scraped_data[0]['email']=None\n",
        "                                print(\"Site is temporarily down!\")\n",
        "                        except requests.exceptions.ConnectionError:\n",
        "                            emailresponse.status_code = \"Connection refused\"\n",
        "                    else:\n",
        "                        scraped_data[0]['email']=None\n",
        "                        print('URL & Email Not Found!!')\n",
        "                else:\n",
        "                    updated=False\n",
        "                if updated:\n",
        "                        writer.writerow(scraped_data[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCMtbweEUjjw"
      },
      "source": [
        "From the scraped data,we need to find email for the website using regex pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Qk1unnUsYP"
      },
      "source": [
        "After finding the email,we able to write the records in yellowpages-scraped-data.csv file."
      ]
    }
  ]
}